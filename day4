선형회귀
    - 종속변수 y와 한 개 이상의 독립변수 x와의 선형 상관관계를 모델링하는 회귀분석 기법
    - 기존 데이터를 활용해 연속형 변수값을 예측
    - y = ax + b 꼴의 수식을 만들고 a와 b의 값을 찾아냄
    - 하나 이상의 특성과 연속적인 타깃 변수 사이의 관계를 모델링하는 것이 목적
    - 지도 학습의 회귀는 범주형 클래스 레이블이 아니라 연속적인 출력 값을 예측

    단순 선형 회귀
        - 단순 선형 회귀는 하나의 특성과 연속적인 타깃 사이의 관계를 모델링
        - y = w0 + w1x
        - 여기서 w0는 y축 절편을 나타내고 w1은 특성의 가중치
        - 특성과 타킷 사이의 관계를 나타내는 선형 방정식의 가중치를 학습하는 것이 목적
        - 이 방정식으로 훈련 데이터셋이 아닌 새로운 샘플의 타깃 값을 예측할 수 있음
        - 데이터에 가장 잘 맞는 직선을 회귀 직선이라고 함
        - 회귀 직선과 훈련 샘플 사이의 직선 거리를 오프셋 또는 예측 오차인 잔차라고 함
    
    예측 함수와 실제값 간의 차이
        - 예측 함수는 예측값과 실제값 간의 차이를 최소화하는 방향으로
        - 데이터 n개 중 i번째 데이터의 y값에 대한 실제값과 예측값의 차이
        - 데이터가 5개 있을 때 5개 데이터의 오차의 합
        - 오차 값들이 음수나 양수로 나왔을 때 값들 간의 차이가 상쇄되어 0으로 계산될 수 있음
    
    비용함수(cost function)
        - 머신러닝에서 최소화해야 할 예측값과 실제값의 차이
    
    가설함수(hypothesis function)
        - 예측값을 예측하는 함수
    
    최소자승법(least square method)
        - 선형대수의 표기법을 사용하여 방정식으로 선형회귀 문제를 푸는 방법
        - 데이터의 개수가 피쳐의 개수보다 많은 경우가 대부분이라서 자주 사용됨
        - 장점 : 반복과 사용자가 지정하는 하이퍼 매개변수가 존재하지 않아서 데이터만 있으면 쉽게 해를 구할 수 있음
        - 단점 : 피쳐가 늘어나면 속도가 느려짐
    
    경사하강법(gradient descent)
        - 경사를 하강하면서 수식을 최소화하는 매개변수의 값을 찾아내는 방법
        - 점이 최솟값을 달성하는 방향으로 점점 내려감
        - 경사 : 경사하강법의 하이퍼 매개변수
    
    경사하강법에서 개발자가 결정해야 할 것
        - 학습률(learning rate)을 얼마로 할 것인가? 알파 값을 결정
            * 반복이 수행될 때마다 최솟값 변화
            * 값이 너무 작으면 충분히 많은 반복을 적용해도 원하는 최적값을 찾지 못하는 경우 발생
            * 값이 너무 크면 발산하여 최솟값 수렴 않거나 시간이 너무 오래 걸림
        
    훈련/테스트 분할
        - 머신러닝에서 데이터를 학습을 하기 위한 학습 데이터셋과 학습의 결과로 생성된 모델의 성능을 평가하기 위한 테스트 데이터셋으로 나눔
        - 모델이 새로운 데이터셋에도 일반화하여 처리할 수 있는지를 확인
    
    모델이 데이터에 과다적합(over-fit)된 경우
        - 생성된 모델이 특정 데이터에만 잘 맞아서 해당 데이터셋에 대해서는 성능을 발휘할 수 있지만 새로운 데이터셋에서는 전혀 성능을 낼 수 없다
    
    모델이 데이터에 과소적합(under-fit)된 경우
        - 기존 학습 데이터를 제대로 예측하지 못함
    
    홀드아웃 메서드
        - 전체 데이터셋에서 일부를 학습 데이터와 테스트 데이터로 나누는 일반적인 데이터 분할 기법
            * 전체 데이터에서 랜덤하게 학습 데이터셋과 테스트 데이터셋을 나눔
            * 일반적으로 7:3, 8:2 정도의 비율
            * sklearn 모듈이 제공하는 train_test_split 함수
    
    경사하강법 종류
        - 전체-배치 경사하강법(full-batch gradient descent)
        - 확률적 경사하강법(Stochasitc Gradient Decent, SGD
        - 미니-배치 경사하강법(mini-batch Gradient descent)
    
    전체-배치 경사하강법
        - 모든 데이터를 한 번에 입력하는 경사하강법
        - 하나의 값에 대한 경사도를 구한 다음 값들을 업데이트
        - 업데이터 횟수 감소
            * 가중치 업데이트 횟수가 줄어 계산상 효율성 상승
        - 안정적인 비용함수 수렴
            * 모든 값의 평균을 구하기 때문에 일반적으로 경사하강법이 갖는 지역 최적화 문제를 만날 가능성도 있음
        - 업데이트 속도 증가
            * 대규모 데이터셋을 한 번에 처리하면 모델의 매개변수 업데이트 속도에 문제 발생이 적어짐
            * 데이터가 백만 단위 이상을 넘어가면 하나의 머신에서는 ㅓ리가 불가능해져서 메모리 문제가 발생
        
    확률적 경사하강법 
        - 학습용 데이터에서 샘플들을 랜덤하게 뽑아서 사용
        - 대상 데이터를 섞은 후, 일반적인 경사하강법처럼 데이터를 한 개씩 추출하여 가중치 업데이트
        - 빈번한 업데이트를 하기 때문에 데이터 분석가가 모델의 성능 변화를 빠르게 확인
        - 데이터의 특성에 따라 훨씬 더 빠르게 결과값을 냄
        - 지역 최적화를 회피
        - 대용량 데이터를 사용하는 경우 시간이 매우 오래 걸림
        - 결과의 마지막 값을 확인하기 어려움
    
    미니-배치 경사하강법
        - 데이터의 랜덤한 일부분만 입력해서 경사도 평균을 구해 가중치 업데이트
    
    에포크(epoch)
        - 데이터를 한 번에 모두 학습시키는 횟수
        - 전체-배치 SDG를 한 번 학습하는 루피가 실행될 때 1에포크의 데이터가 학습된다고 말함
    
    배치 사이즈(batch-size)
        - 한번에 학습되는 데이터의 개수
        - 총 데이터가 5012개 있고 배치 사이즈가 512라면 10번의 루프가 돌면서 1에포크를 학습했다고 말함
    
    편향(bias)
        - 학습된 모델이 학습 데이터에 대해 만들어 낸 예측값과 실제값과의 차이
        - 모델의 결과가 얼마나 한쪽으로 쏠려 있는지 나타냄
        - 편향이 크면 학습이 잘 진행되기는 했지만 해당 데이터에만 잘 맞음
    
    분산(variance)
        - 학습된 모델이 테스팅 데이터에 대해 만들어 낸 예측값과 실제값과의 차이
        - 모델의 결과가 얼마나 퍼져 있는지 나타냄
    
    리지 회귀(ridge regression)
        - L2 정규화
        - 놈(norm)
        - 좌표평면의 원점에서 점까지의 거리를 나타내어 벡터의 크기를 측정하는 기법
        - x는 하나의 벡터
        - L2 놈 : 벡터 간 원소들의 제곱합에 제곱근을 취함
    
    라쏘 회귀(lasso regression)
        - L1 정규화라고 부름
        - 가중치에 페널티텀을 추가하는데, 기존 수식에다 L1 놈 페널티를 추가하여 계산
        - L1 놈 : 절대값을 사용하여 거리를 측정