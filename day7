의사결정트리
    의사결정트리(decision tree)
        - 어떤 규칙을 하나의 트리(tree) 형태로 표현한 후 이를 바탕으로 분류나 회귀 문제를 해결
        - 규칙은 'if-else' 문으로 표현이 가능
        - 트리는 일종의 경로를 표현하는 것
        - 트리 구조의 마지막 노드에는 분류 문제에서 클래스, 회귀 문제에서는 예측치가 들어감
    
    의사결정트리는 딥러닝 기반을 제외한 전통적인 통계 기반의 머신러닝 모델 중 효과와 실용성이 가장 좋음
        - 테이블형 데이터에 있어 설명력과 성능의 측면에서 딥러닝 모델들과 대등하게 경쟁
        - 앙상블 모델이나 부스팅같은 새로운 기법들이 모델들의 성능을 대폭 향상시키고 있음
    
    분할 속성(splitting attributes)
        - 부모 노드에 들어가는 if-else문의 조건들
        - 어떤 분할 속성이 가장 모호성을 줄일 것인지 파악
    
    엔트로피
        - 어떤 목적 달성을 위한 경우의 수를 정략적으로 표현하는 수치
        - 현재의 정보 제공 상태를 측정
        - 어떤 분할 속성을 선택하였을 때 정보를 제공하는 기준값을 정하고, 그 값을 최소화 또는 최대화하는 방향으로 알고리즘 실행

    낮은 엔트로피 = 경우의 수가 적음 = 낮은 불확실성
    높은 엔트로피 = 경우의 수가 높음 = 높은 불확실성

    정보이득(information gain)
        - 엔트로피를 사용하여 속성별 분류 시 데이터가 얼마나 순수한지를 측정하는 지표
        - 각 속성을 기준으로 데이터를 분류했을 때 엔트로피를 측정
        * 전체 엔트로피 - 속성별 엔트로피 = 속성별 정보 이득
    
    성장(grow)
        - 일반적으로 의사결정트리를 생성하는 방법을 성장이라고 부름
        - 트리를 성장시키는 개념
    
    ID3(iterative Dichotomiser 3)
        - 반복적으로 데이터를 나누는 알고리즘
        - 톱다운 방식으로 데이터를 나누면서 탐욕적으로 현재 상태에서 최적화를 추진하는 방법을 선택
    
    의사결정 알고리즘의 특징
    1. 재귀적 작동
        - 가지가 되는 속성을 선택한 후 해당 가지로 데이터를 나누면, 이전에 적용되었던 알고리즘이 남은 데이터에 적용됨
        - 남은 데이터에서만 최적의 모델을 찾는 방법으로 작동
    
    2. 속성 기준으로 가지치기 수행
        - 가장 불확실성이 적은 속성을 기준으로 가지치기를 수행
    
    3. 중요한 속성 정보 제공
        - 처음 분리 대상이 되는 속성이 가장 중요한 속성
    
    의사결정트리의 장점
    1. 불필요한 속성값에 대한 스케일링
        - 전처리 단계 없이 바로 사용할 수 있다
    
    2. 강건(robust)한 이상치(outlier)
        - 관측치의 절대값이 아닌 순서가 중요하기 때문에 필요 이상으로 엄청 큰 값이나 작은 값에 대해서도 분류 성능이 크게 떨어지지 않는다

    3. 자동적인 변수 선택
        - 알고리즘에 의해 중요한 변수들이 우선적으로 선택되어 조금 더 손쉽게 중요한 속성을 확인할 수있다. 의사결정트리 계열의 알고리즘이 가지고 있는 가장 큰 장점 중 하나이다  
    
    정보이득의 문제점
        - 수식의 특성상 속성의 값이 다양할수록 선택의 확률이 높아지는 문제가 발생
        - 데이터가 매우 많고 속성이 다양할 때 위 수식의 |Dj|/|D| 값이 작아짐
        - 해당 속성의 엔트로피가 낮아져 단순히 속성 안에 있는 값의 종류를 늘리는 것만으로 정보 이득이 높아짐
    
    c4.5
        - 정보 이득을 측정하는 방식을 좀 더 평준화시켜 단순한 정보 값을 대신 사용
        - 기존 정보 이득의 분모에 평준화 함수 SplitInfo 추가
        - 정교한 불순도 지표 활용
        - 범주형 변수 뿐 아니라 연속형 변수를 사용 가능
        - 결측치가 포함된 데이터도 사용 가능
        - 과적합을 방지하기 위한 가지치기
    
    지니 지수
        - 경제학에서 소득의 불평등도를 측정할 때 사용하는 지표
        - 의사결정트리에서 각 속성의 불순도를 측정하는 방법으로 사용
    
    이진 분할
        - CART 알고리즘의 핵심은 불확실성을 측정하는 기준 값이 엔트로피에서 지니 지수로 바뀐 것
        - 구현 측면에서 가장 큰 차이점은 이진 분할을 실시한다는 것
    
    트리 가지치기(tree pruning)
        - 의사결정트리의 마지막 노드의 개수를 지정하여 트리의 깊이를 조정하는 방법
    
    사전 가지치기(pre-pruning)
        - 처음 트리를 만들 때 트리의 깊이나 마지막 노드의 최소 개수 등을 사전에 결정하여 입력
        - 데이터 분석가가 하이퍼 매개변수로 모든 값을 입력해야 하는 점이 어려움
        - 계산 효율이 좋고 작은 데이터셋에서도 쉽게 작동
        - 사용자가 중요한 속성 값을 놓치거나 과소적합 문제가 발생할 수 있다
    
    사후 가지치기(post-pruning)
        - 트리를 먼저 생성한 후 실험적으로 하이퍼 매개변수를 조정
        - 하나의 지표를 정해두고 실험적으로 다양한 하이퍼 매개변수를 조정하며 최적의 값을 찾음
        - '최종 노드의 개수', '트리의 깊이', '선택되는 속성의 개수' 등을 하이퍼 매개변수로 조정하며 성능을 비교
        - 먼저 전체 데이터를 훈련셋, 검증셋, 테스트셋으로 분류하고, 훈련셋과 테스트셋의 성능을 비교
    
    연속형 데이터를 나누는 기준
        - 모든 데이터를 기준점으로 하여 데이터를 나누기
            * 너무 많은 기준점이 생겨 과대적합 문제가 발생하거나 분류의 정확도가 떨어짐
        - 통계적 수치로 중위값이나 4분위수를 기준점으로 나누기
            * 25%씩 데이터를 나눠서 분류 기준을 변경
            * 과소적합 문제가 발생하여 분류의 성능을 떨어뜨릴 수 있음
        - 가장 많이 쓰는 방법으로, Y 클래스의 값을 기준으로 해당 값이 변할 때를 기준점으로 삼아 분기
        
